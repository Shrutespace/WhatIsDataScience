---
title: "CaseStudy2"
author: "Clayton Moore"
date: "November 9th, 2015"
output: 
  html_document:
    keep_md: true
---
## Case Study II: Using Data Science to Explain Data Science by Clay Moore

The point of this assignment was to "harvest" job skills from the website Cybercoders, an online job site that specialzies in jobs relating to coding.  The first step was to create a code that was able to extract just the skill section from numerous individual job webpages.  The code located below does that.  The code was supplied by Dr. Monnie McGee of SMU, as well as classmates from the Introduction to Data Science class.  

```{r getFreeForm}
library(XML)
library(RCurl)
StopWords = readLines("http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop")

asWords = function(txt, stopWords = StopWords, stem = FALSE)
{
  words = unlist(strsplit(txt, '[[:space:]!.,;#:()/"]+'))
  words = words[words != ""]
  if(stem && require(Rlibstemmer))
     words = wordStem(words)
  i = tolower(words) %in% tolower(stopWords)
  words[!i]
}

removeStopWords = function(x, stopWords = StopWords) 
     {
         if(is.character(x))
             setdiff(x, stopWords)
         else if(is.list(x))
             lapply(x, removeStopWords, stopWords)
         else
             x
     }

cy.getFreeFormWords = function(doc, stopWords = StopWords)
     {
         nodes = getNodeSet(doc, "//div[@class='job-details']/
                                 div[@data-section]")
         if(length(nodes) == 0) 
             nodes = getNodeSet(doc, "//div[@class='job-details']//p")
         
         if(length(nodes) == 0) 
             warning("did not find any nodes for the free form text in ",
                     docName(doc))
         
         words = lapply(nodes,
                        function(x)
                            strsplit(xmlValue(x), 
                                     "[[:space:][:punct:]]+"))
         
         removeStopWords(words, stopWords)
     }

```

```{r Question1}
cy.getSkillList = function(doc)
{
  lis = getNodeSet(doc, "//div[@class = 'skills-section']//
                         li[@class = 'skill-item']//
                         span[@class = 'skill-name']")

  sapply(lis, xmlValue)
}

cy.getDatePosted = function(doc)
  { xmlValue(getNodeSet(doc, 
                     "//div[@class = 'job-details']//
                        div[@class='posted']/
                        span/following-sibling::text()")[[1]],
    trim = TRUE) 
}

cy.getLocationSalary = function(doc)
{
  ans = xpathSApply(doc, "//div[@class = 'job-info-main'][1]/div", xmlValue)
  names(ans) = c("location", "salary")
  ans
}

# cy.getSkillList(cydoc)
# cy.getLocationSalary(cydoc)
```

```{r cy.readPost}
cy.readPost = function(u, stopWords = StopWords, doc = htmlParse(u))
  {
    ans = list(words = cy.getFreeFormWords(doc, stopWords),
         datePosted = cy.getDatePosted(doc),
         skills = cy.getSkillList(doc))
    o = cy.getLocationSalary(doc)
    ans[names(o)] = o
    ans
}
# cyFuns = list(readPost = function(u, stopWords = StopWords, doc=htmlParse(u)))
```

```{r GetPosts}
# Obtain URLs for job posts
txt = getForm("http://www.cybercoders.com/search/", searchterms = '"Data Scientist"',
              searchlocation = "",  newsearch = "true", sorttype = "")
# Parse the links
doc = htmlParse(txt, asText = TRUE)
links = getNodeSet(doc, "//div[@class = 'job-title']/a/@href")
# Save the links in the vector joblinks
joblinks <- getRelativeURL(as.character(links), "http://www.cybercoders.com/search/")
# Read the posts
posts <- lapply(joblinks,cy.readPost)

cy.getPostLinks = function(doc, baseURL = "http://www.cybercoders.com/search/") 
  {
    if(is.character(doc)) doc = htmlParse(doc)
    links = getNodeSet(doc, "//div[@class = 'job-title']/a/@href") 
    getRelativeURL(as.character(links), baseURL)
}

cy.readPagePosts = function(doc, links = cy.getPostLinks(doc, baseURL),
baseURL = "http://www.cybercoders.com/search/")
  {
    if(is.character(doc)) doc = htmlParse(doc)
    lapply(links, cy.readPost)
 }

## Testing the function with the parsed version of the first page of results in object doc
posts = cy.readPagePosts(doc)
sapply(posts,`[[`, "salary")
summary(sapply(posts, function(x) length(unlist(x$words))))
```

```{r Next Page of Results}
# Test of concept
# getNodeSet(doc, "//a[@rel='next']/@href")[[1]]
## A function to get all pages
cy.getNextPageLink = function(doc, baseURL = docName(doc))
{
  if(is.na(baseURL))
     baseURL = "http://www.cybercoders.com/"
  link = getNodeSet(doc, "//li[@class = 'lnk-next pager-item ']/a/@href")
  if(length(link) == 0)
    return(character())
    link2 <- gsub("./", "search/",link[[1]])
 getRelativeURL(link2, baseURL)
}

# Test the above function
tmp = cy.getNextPageLink(doc, "http://www.cybercoders.com")
```

```{r cyberCoders}
cyberCoders =
function(query)
{
   txt = getForm("http://www.cybercoders.com/search/",
                  searchterms = query,  searchlocation = "",
                  newsearch = "true",  sorttype = "")
   doc = htmlParse(txt)

   posts = list()
   while(TRUE) {
       posts = c(posts, cy.readPagePosts(doc))
       nextPage = cy.getNextPageLink(doc)
       if(length(nextPage) == 0)
          break

       nextPage = getURLContent(nextPage)
       doc = htmlParse(nextPage, asText = TRUE)
   }
   invisible(posts)
}
```

```{r Get Skills}
dataSciPosts <- cyberCoders("Data Scientist")
tt = sort(table(unlist(lapply(dataSciPosts, `[[`, "skills"))),
           decreasing = TRUE)
FinalSkills<-tt[tt >= 2]
```

Now this is where my coding takes over.  I observed the FinalSkills array and located a few quick changes I could make to combine and correct a few categories to make a visual graphic more suitable.  I looked online to understand what some of the skills meant, and made changes based on that.  

```{r Combine the Data}
##Big Data Combination
BigData1= grepl("^Big Data+",names(FinalSkills))
BigData2=grepl("Data Mining",names(FinalSkills))
if(any(BigData1|BigData2))names(FinalSkills)[BigData1|BigData2]="Big Data"
##Data Visualization Combination
Visual=grepl("Vi+", names(FinalSkills))
Visual1=grepl("Tableau",names(FinalSkills))
if(any(Visual|Visual1))names(FinalSkills)[Visual|Visual1]="Data Visualization"
#Java Combination
Java=grepl("Java|HDFS",names(FinalSkills))
if(any(Java))names(FinalSkills)[Java]="Java Programming"
#Hadoop Combination
Hadoop<-grepl("Hi+|Hadoop|Mahout",names(FinalSkills))
if(any(Hadoop))names(FinalSkills)[Hadoop]="Hadoop"
#MapReduce Combination
MapReduce<-grepl("Map+",names(FinalSkills))
#Machine Learning & Algorithms Combination
ML<-grepl("Machine+|Algorithm+",names(FinalSkills))
if(any(ML))names(FinalSkills)[ML]="Machine Learning"
#Statisitcs Combination
Stat<-grepl("Statist+|statis+",names(FinalSkills))
if(any(Stat))names(FinalSkills)[Stat]="Statistical Knowledge"
#Python Combination
Pyt<-grepl("Python",names(FinalSkills))
if(any(Pyt))names(FinalSkills)[Pyt]="Python"
```

After setting like terms, I still had to combine them together, which is what the next strand of code explains.  I changed the data into a data frame, then combined one of the column values, and organized them by descending values

```{R Combining Like Terms}
library(plyr)
FinalSkillsTable<-as.data.frame.table(FinalSkills)
Final<-ddply(FinalSkillsTable,"Var1",numcolwise(sum))
Final<-Final[order(-Final$Freq),]
```

Finally, I created a wordcloud with the results.  I cut out the minimum amount to 4 after combining some of the like terms because these would be the most relevant skills for Data Scientists
```{R WordCloud}
library(wordcloud)
wordcloud(words=Final$Var1, freq=Final$Freq,min.freq=4,colors=brewer.pal(8,"Dark2"),random.order = FALSE)
```

From the word cloud, we can see that Big Data is one of the biggest contributors to what skills a Data Scientist needs.  Big Data is followed by Python, Statistical Knowledge, R, and Hadoop.  A job as a data scientist seems to require a lot of coding and software programming, and not as much domain expertise and data visualization.